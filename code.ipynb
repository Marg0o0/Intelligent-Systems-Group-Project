{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aec53e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d9e4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id                 artists  \\\n",
      "0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
      "1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
      "2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
      "3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
      "4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
      "\n",
      "                                          album_name  \\\n",
      "0                                             Comedy   \n",
      "1                                   Ghost (Acoustic)   \n",
      "2                                     To Begin Again   \n",
      "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
      "4                                            Hold On   \n",
      "\n",
      "                   track_name  popularity  duration_ms  explicit  \\\n",
      "0                      Comedy          73       230666     False   \n",
      "1            Ghost - Acoustic          55       149610     False   \n",
      "2              To Begin Again          57       210826     False   \n",
      "3  Can't Help Falling In Love          71       201933     False   \n",
      "4                     Hold On          82       198853     False   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
      "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
      "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
      "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
      "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
      "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
      "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
      "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
      "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
      "4          0.000000    0.0829    0.167  119.949               4    acoustic  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114000, 20)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Acesso ao dataset\n",
    "music = pd.read_csv('dataset.csv', index_col=0) #chama o ficheiro e remove coluna desnecessária (index_col)\n",
    "\n",
    "#Garantir que o acesso foi bem sucedido (faz print das primeiras 5 linhas do dataset)\n",
    "print(music.head())\n",
    "music.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a541e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes da limpeza: 32656 músicas duplicadas\n",
      "(81344, 20)\n",
      "Depois de filtrar para incluir apenas as músicas: (80483, 20)\n",
      "track_id            0\n",
      "artists             1\n",
      "album_name          1\n",
      "track_name          1\n",
      "popularity          0\n",
      "duration_ms         0\n",
      "explicit            0\n",
      "danceability        0\n",
      "energy              0\n",
      "key                 0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "acousticness        0\n",
      "instrumentalness    0\n",
      "liveness            0\n",
      "valence             0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "track_genre         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##limpeza do dataset\n",
    "# Contar quantas linhas duplicadas existem (mesmo nome + mesmo artista)\n",
    "duplicadas_antes = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Antes da limpeza: {duplicadas_antes} músicas duplicadas\")\n",
    "\n",
    "# Remover duplicados (mesmo nome e mesmo artista)\n",
    "music = music.drop_duplicates(subset=[\"track_name\", \"artists\"], keep=\"first\")\n",
    "\n",
    "print(music.shape)\n",
    "\n",
    "#remover tracks que não são musica (podcasts, audio books)\n",
    "music = music[music['speechiness'] <= 0.66]\n",
    "\n",
    "print(f'Depois de filtrar para incluir apenas as músicas: {music.shape}')\n",
    "\n",
    "\n",
    "#valores em falta por coluna\n",
    "print(music.isnull().sum()) \n",
    "music = music.dropna()  #por serem poucos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1084acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.715 0.267 0.12  ... 0.743 0.413 0.708]\n",
      "matriz x \n",
      " [[6.76000e-01 4.61000e-01 1.00000e+00 ... 1.01000e-06 8.79170e+01\n",
      "  4.00000e+00]\n",
      " [4.20000e-01 1.66000e-01 1.00000e+00 ... 5.56000e-06 7.74890e+01\n",
      "  4.00000e+00]\n",
      " [4.38000e-01 3.59000e-01 0.00000e+00 ... 0.00000e+00 7.63320e+01\n",
      "  4.00000e+00]\n",
      " ...\n",
      " [6.29000e-01 3.29000e-01 0.00000e+00 ... 0.00000e+00 1.32378e+02\n",
      "  4.00000e+00]\n",
      " [5.87000e-01 5.06000e-01 7.00000e+00 ... 0.00000e+00 1.35960e+02\n",
      "  4.00000e+00]\n",
      " [5.26000e-01 4.87000e-01 1.00000e+00 ... 0.00000e+00 7.91980e+01\n",
      "  4.00000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80482, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defenir as features\n",
    "f0 = music['danceability'].values      #[0,1]\n",
    "f1 = music['energy'].values            #[0,1]\n",
    "f2 = music['key'].values               #[-1,11]\n",
    "f3 = music['loudness'].values          #dB\n",
    "f4 = music['mode'].values              #binary\n",
    "f5 = music['acousticness'].values      #[0,1]\n",
    "f6 = music['instrumentalness'].values  #[0,1]\n",
    "f7 = music['tempo'].values             #bpm\n",
    "f8 = music['time_signature'].values    #[0,5]\n",
    "\n",
    "f9 = music['track_genre'].values       #classes\n",
    "y = music['valence'].values         #valence-target\n",
    "\n",
    "'---------------------------------------'\n",
    "# matriz features\n",
    "#track genre não incluida, só mais á frente\n",
    "X = music[['danceability', 'energy', 'key', 'loudness', 'mode', 'acousticness', 'instrumentalness', 'tempo', 'time_signature']].values    \n",
    "\n",
    "#x = np.column_stack((f0, f1, f2, f3, f4, f5, f6, f7, f8))\n",
    "#x = np.c_[f0, f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "\n",
    "'-------------------------------------melhor metodo?'\n",
    "\n",
    "print(y)\n",
    "print(f'matriz x \\n {X}')\n",
    "\n",
    "music.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c21e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depois da limpeza: 0 músicas duplicadas\n"
     ]
    }
   ],
   "source": [
    "#verificaçaõ da limpeza\n",
    "duplicadas_depois = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Depois da limpeza: {duplicadas_depois} músicas duplicadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18f9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test será o conjunto nunca visto pelo algoritmo até ao fim'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train, validation e test split\n",
    "test_size = 0.3\n",
    "val_size = 0.5   #será metade dos dados de teste\n",
    "#Seprarar o conjunto de treino (70%) e o temporário (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "#Separar o conjunto temporário em teste (15%) e validação (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = val_size, random_state = 42)\n",
    "\n",
    "'test será o conjunto nunca visto pelo algoritmo até ao fim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f91ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.97000000e-01 2.94000000e-01 4.54545455e-01 ... 0.00000000e+00\n",
      "  7.14926121e-01 4.00000000e+00]\n",
      " [5.86000000e-01 6.95000000e-01 9.09090909e-02 ... 0.00000000e+00\n",
      "  6.48883191e-01 4.00000000e+00]\n",
      " [6.67000000e-01 5.03000000e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.91748434e-01 4.00000000e+00]\n",
      " ...\n",
      " [7.88000000e-01 7.45000000e-01 1.81818182e-01 ... 8.50000000e-01\n",
      "  5.01294315e-01 4.00000000e+00]\n",
      " [5.64000000e-01 7.31000000e-01 8.18181818e-01 ... 3.48000000e-06\n",
      "  3.90496853e-01 4.00000000e+00]\n",
      " [7.71000000e-01 7.39000000e-01 8.18181818e-01 ... 0.00000000e+00\n",
      "  5.34260309e-01 4.00000000e+00]]\n",
      " matriz x-val: \n",
      " [[8.18000000e-01 7.09000000e-01 0.00000000e+00 ... 4.24000000e-02\n",
      "  5.25845208e-01 4.00000000e+00]\n",
      " [6.92000000e-01 8.57000000e-01 1.81818182e-01 ... 1.43000000e-01\n",
      "  4.49423105e-01 4.00000000e+00]\n",
      " [5.13000000e-01 9.16000000e-01 9.09090909e-02 ... 1.72000000e-05\n",
      "  5.01281988e-01 4.00000000e+00]\n",
      " ...\n",
      " [7.88000000e-01 7.45000000e-01 1.81818182e-01 ... 8.50000000e-01\n",
      "  5.01294315e-01 4.00000000e+00]\n",
      " [5.64000000e-01 7.31000000e-01 8.18181818e-01 ... 3.48000000e-06\n",
      "  3.90496853e-01 4.00000000e+00]\n",
      " [7.71000000e-01 7.39000000e-01 8.18181818e-01 ... 0.00000000e+00\n",
      "  5.34260309e-01 4.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'----------PORQUE NÃO POSSO FAZER A NORMALIZAÇÃO ANTES DE SEPARAR ENTRE TESTE E TREINO\\nOU SEJA DÁ PARA O FAZER SEM LEVAR A OVER FITTING?\\n\\nSobre o comentário (importante!)\\n\\n“Por que não posso fazer a normalização antes de separar treino e teste?”\\n\\nPorque se fizeres isso, o MinMaxScaler vai usar informação do conjunto de validação/teste para definir os limites (mínimo e máximo).\\nIsso causa data leakage (vazamento de dados) e overfitting indireto, pois o modelo “vê” dados que não deveria antes do treino.\\n\\n👉 O correto é:\\n\\nSeparar primeiro treino/validação/teste\\n\\nAjustar (fit) o scaler só no treino\\n\\nAplicar (transform) o mesmo scaler aos outros conjuntos.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizar varivaeis Key, loudness, tempo, time_signature\n",
    "scaler = MinMaxScaler()\n",
    "X_train[ :, [ 2, 3, 6, 7]] = scaler.fit_transform(X_train[ :, [ 2, 3, 6, 7]])\n",
    "X_val[ :, [ 2, 3, 6, 7]] = scaler.transform(X_val[ :, [2, 3, 6, 7]])\n",
    "\n",
    "print(X_train)\n",
    "print(f' matriz x-val: \\n {X_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreEncoder(nn.Module):\n",
    "    def __init__(self, num_genres, embedding_dim):\n",
    "        super(GenreEncoder, self).__init__()\n",
    "        self.genre_embedding = nn.Embedding(num_genres, embedding_dim)\n",
    "\n",
    "    def forward(self, genre_idx):\n",
    "        # genre_idx: [batch_size, 1] ou [batch_size]\n",
    "        z = self.embedding(genre_idx).squeeze(1)\n",
    "        return z  # z ∈ R^(batch_size × embedding_dim)\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_hat = self.model(x)\n",
    "        return z_hat  # ẑ ∈ R^(batch_size × embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1253fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "embedding_dim = 30\n",
    "num_genres = 114\n",
    "input_dim = X_train.shape[1]\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "dropout = 0.2     #nunca acima de 0.5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "genre_embedding = GenreEncoder(num_genres, embedding_dim)\n",
    "model = FeatureEncoder(input_dim, embedding_dim)\n",
    "#Loss\n",
    "criterion = nn.CosineSimilarity(dim=1)\n",
    "#Otimizer\n",
    "optimizer = optim.Adam( list(model.parameters()) + list(genre_embedding.parameters()), lr=1e-3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # z do género\n",
    "    z = genre_embedding ( x_genre )       # [batch, embedding_dim]\n",
    "    # ẑ das features\n",
    "    z_hat = model (x_feat)  # [batch, embedding_dim]\n",
    "\n",
    "    # Loss = 1 - cos_sim\n",
    "    cos_sim = criterion(z_hat, z)\n",
    "    loss = 1 - cos_sim.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fa761",
   "metadata": {},
   "source": [
    "numero de layers ? como escolho\n",
    "posso usar o 64 tambem ou é melhor optar por algo mais elabora devido a um data set maior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a515ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversão dos dados para tensores pythorch\n",
    "X_train = torch.tensor (X_train, dtype = torch.float32)\n",
    "y_train = torch.tensor (y_train, dtype = torch.float32)\n",
    "X_val = torch.tensor (X_val, dtype = torch.float32)\n",
    "y_val = torch.tensor (y_val, dtype = torch.float32)\n",
    "\n",
    "#unir tensores X_train e y_train num dataset\n",
    "train_dataset = TensorDataset (X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
