{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aec53e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d9e4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id                 artists  \\\n",
      "0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
      "1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
      "2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
      "3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
      "4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
      "\n",
      "                                          album_name  \\\n",
      "0                                             Comedy   \n",
      "1                                   Ghost (Acoustic)   \n",
      "2                                     To Begin Again   \n",
      "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
      "4                                            Hold On   \n",
      "\n",
      "                   track_name  popularity  duration_ms  explicit  \\\n",
      "0                      Comedy          73       230666     False   \n",
      "1            Ghost - Acoustic          55       149610     False   \n",
      "2              To Begin Again          57       210826     False   \n",
      "3  Can't Help Falling In Love          71       201933     False   \n",
      "4                     Hold On          82       198853     False   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
      "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
      "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
      "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
      "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
      "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
      "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
      "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
      "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
      "4          0.000000    0.0829    0.167  119.949               4    acoustic  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114000, 20)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Acesso ao dataset\n",
    "music = pd.read_csv('dataset.csv', index_col=0) #chama o ficheiro e remove coluna desnecessária (index_col)\n",
    "\n",
    "#Garantir que o acesso foi bem sucedido (faz print das primeiras 5 linhas do dataset)\n",
    "print(music.head())\n",
    "music.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a541e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes da limpeza: 32656 músicas duplicadas\n",
      "(81344, 20)\n",
      "Depois de filtrar para incluir apenas as músicas: (80483, 20)\n",
      "track_id            0\n",
      "artists             1\n",
      "album_name          1\n",
      "track_name          1\n",
      "popularity          0\n",
      "duration_ms         0\n",
      "explicit            0\n",
      "danceability        0\n",
      "energy              0\n",
      "key                 0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "acousticness        0\n",
      "instrumentalness    0\n",
      "liveness            0\n",
      "valence             0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "track_genre         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##limpeza do dataset\n",
    "# Contar quantas linhas duplicadas existem (mesmo nome + mesmo artista)\n",
    "duplicadas_antes = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Antes da limpeza: {duplicadas_antes} músicas duplicadas\")\n",
    "\n",
    "# Remover duplicados (mesmo nome e mesmo artista)\n",
    "music = music.drop_duplicates(subset=[\"track_name\", \"artists\"], keep=\"first\")\n",
    "\n",
    "print(music.shape)\n",
    "\n",
    "#remover tracks que não são musica (podcasts, audio books)\n",
    "music = music[music['speechiness'] <= 0.66]\n",
    "\n",
    "print(f'Depois de filtrar para incluir apenas as músicas: {music.shape}')\n",
    "\n",
    "\n",
    "#valores em falta por coluna\n",
    "print(music.isnull().sum()) \n",
    "music = music.dropna()  #por serem poucos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1084acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acoustic' 'acoustic' 'acoustic' ... 'world-music' 'world-music'\n",
      " 'world-music']\n",
      "matriz x \n",
      " [[6.76000e-01 4.61000e-01 1.00000e+00 ... 7.15000e-01 8.79170e+01\n",
      "  4.00000e+00]\n",
      " [4.20000e-01 1.66000e-01 1.00000e+00 ... 2.67000e-01 7.74890e+01\n",
      "  4.00000e+00]\n",
      " [4.38000e-01 3.59000e-01 0.00000e+00 ... 1.20000e-01 7.63320e+01\n",
      "  4.00000e+00]\n",
      " ...\n",
      " [6.29000e-01 3.29000e-01 0.00000e+00 ... 7.43000e-01 1.32378e+02\n",
      "  4.00000e+00]\n",
      " [5.87000e-01 5.06000e-01 7.00000e+00 ... 4.13000e-01 1.35960e+02\n",
      "  4.00000e+00]\n",
      " [5.26000e-01 4.87000e-01 1.00000e+00 ... 7.08000e-01 7.91980e+01\n",
      "  4.00000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80482, 20)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defenir as features\n",
    "f0 = music['danceability'].values      #[0,1]\n",
    "f1 = music['energy'].values            #[0,1]\n",
    "f2 = music['key'].values               #[-1,11]\n",
    "f3 = music['loudness'].values          #dB\n",
    "f4 = music['mode'].values              #binary\n",
    "f5 = music['acousticness'].values      #[0,1]\n",
    "f6 = music['instrumentalness'].values  #[0,1]\n",
    "f7 = music['valence'].values           #valence\n",
    "f8 = music['tempo'].values             #bpm\n",
    "f9 = music['time_signature'].values    #[0,5]\n",
    "\n",
    "\n",
    "y = music['track_genre'].values       #classes\n",
    "\n",
    "'---------------------------------------'\n",
    "# matriz features\n",
    "#track genre não incluida, só mais á frente\n",
    "X = music[['danceability', 'energy', 'key', 'loudness', 'mode', 'acousticness', 'instrumentalness', 'valence', 'tempo', 'time_signature']].values    \n",
    "\n",
    "#x = np.column_stack((f0, f1, f2, f3, f4, f5, f6, f7, f8, f9))\n",
    "#x = np.c_[f0, f1, f2, f3, f4, f5, f6, f7, f8, f9]\n",
    "\n",
    "'-------------------------------------melhor metodo?'\n",
    "\n",
    "print(y)\n",
    "print(f'matriz x \\n {X}')\n",
    "\n",
    "music.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c21e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depois da limpeza: 0 músicas duplicadas\n"
     ]
    }
   ],
   "source": [
    "#verificaçaõ da limpeza\n",
    "duplicadas_depois = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Depois da limpeza: {duplicadas_depois} músicas duplicadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e18f9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test será o conjunto nunca visto pelo algoritmo até ao fim'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train, validation e test split\n",
    "test_size = 0.3\n",
    "val_size = 0.5   #será metade dos dados de teste\n",
    "#Seprarar o conjunto de treino (70%) e o temporário (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "#Separar o conjunto temporário em teste (15%) e validação (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = val_size, random_state = 42)\n",
    "\n",
    "'test será o conjunto nunca visto pelo algoritmo até ao fim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c8f91ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.97000000e-01 2.94000000e-01 4.54545455e-01 ... 3.09547739e-01\n",
      "  1.73993000e+02 4.00000000e+00]\n",
      " [5.86000000e-01 6.95000000e-01 9.09090909e-02 ... 9.07537688e-01\n",
      "  1.57920000e+02 4.00000000e+00]\n",
      " [6.67000000e-01 5.03000000e-01 0.00000000e+00 ... 5.86934673e-01\n",
      "  1.44015000e+02 4.00000000e+00]\n",
      " ...\n",
      " [7.88000000e-01 7.45000000e-01 1.81818182e-01 ... 2.00000000e-01\n",
      "  1.22001000e+02 4.00000000e+00]\n",
      " [5.64000000e-01 7.31000000e-01 8.18181818e-01 ... 2.58291457e-01\n",
      "  9.50360000e+01 4.00000000e+00]\n",
      " [7.71000000e-01 7.39000000e-01 8.18181818e-01 ... 6.90452261e-01\n",
      "  1.30024000e+02 4.00000000e+00]]\n",
      " matriz x-val: \n",
      " [[8.86000000e-01 6.26000000e-01 6.36363636e-01 ... 9.48743719e-01\n",
      "  1.24980000e+02 4.00000000e+00]\n",
      " [4.81000000e-01 9.12000000e-01 1.00000000e+00 ... 6.93467337e-01\n",
      "  7.80060000e+01 4.00000000e+00]\n",
      " [5.54000000e-01 8.27000000e-01 1.00000000e+00 ... 1.57788945e-01\n",
      "  1.50104000e+02 4.00000000e+00]\n",
      " ...\n",
      " [8.38000000e-01 8.03000000e-01 5.45454545e-01 ... 5.79899497e-01\n",
      "  1.26026000e+02 4.00000000e+00]\n",
      " [6.32000000e-01 8.83000000e-01 1.00000000e+00 ... 6.64321608e-01\n",
      "  1.00002000e+02 4.00000000e+00]\n",
      " [6.25000000e-01 8.91000000e-01 6.36363636e-01 ... 8.09045226e-01\n",
      "  1.68451000e+02 4.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "#Normalizar varivaeis Key, loudness, tempo, time_signature\n",
    "scaler = MinMaxScaler()\n",
    "X_train[ :, [ 2, 3, 6, 7]] = scaler.fit_transform(X_train[ :, [ 2, 3, 6, 7]])\n",
    "X_val[ :, [ 2, 3, 6, 7]] = scaler.transform(X_val[ :, [2, 3, 6, 7]])\n",
    "\n",
    "print(X_train)\n",
    "print(f' matriz x-val: \\n {X_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87f40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Modelo 1: Encoder do género (y -> z)\n",
    "class GenreEncoder(nn.Module):\n",
    "    def __init__(self, num_genres, embedding_dim):\n",
    "        super(GenreEncoder, self).__init__()\n",
    "        self.genre_embedding = nn.Embedding(num_genres, embedding_dim)\n",
    "\n",
    "    def forward(self, genre_idx):\n",
    "        # genre_idx: [batch_size, 1] ou [batch_size]\n",
    "        z = self.embedding(genre_idx).squeeze(1)\n",
    "        return z  # z ∈ R^(batch_size × embedding_dim)\n",
    "\n",
    "#Modelo 2: Encoder das features (x -> ẑ)\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, dropout_prob = 0.3):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.input = nn.Linear (input_dim, 64)\n",
    "        self.hidden1 = nn.Linear (64, 64)\n",
    "        self.hidden2 = nn.Linear (64, 64)\n",
    "        self.out = nn.Linear (64, embedding_dim)\n",
    "        self.dropout = nn.Droout (p = dropout_prob)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu (self.input(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu (self.hidden1(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = F.relu (self.hidden2(x))\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        z_hat = self.out(x)\n",
    "        return z_hat  # ẑ ∈ R^(batch_size × embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e1253fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup de parâmetros\n",
    "embedding_dim = 30\n",
    "num_genres = 114\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "dropout = 0.2     #nunca acima de 0.5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "017e33af",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#conversão dos dados para tensores pythorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m X_train \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor (X_train, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m X_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor (X_val, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      5\u001b[0m y_val \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor (y_val, dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, complex64, complex128, int64, int32, int16, int8, uint64, uint32, uint16, uint8, and bool."
     ]
    }
   ],
   "source": [
    "#conversão dos dados para tensores pythorch\n",
    "X_train = torch.tensor (X_train, dtype = torch.float32)\n",
    "y_train = torch.tensor (y_train, dtype = torch.float32)\n",
    "X_val = torch.tensor (X_val, dtype = torch.float32)\n",
    "y_val = torch.tensor (y_val, dtype = torch.float32)\n",
    "\n",
    "#unir tensores X_train e y_train num dataset\n",
    "train_dataset = TensorDataset (X_train, y_train)\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593c40",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3417583946.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[86], line 3\u001b[1;36m\u001b[0m\n\u001b[1;33m    model = FeatureEncoder(input_dim = X_train.shape[1], embedding_dim)\u001b[0m\n\u001b[1;37m                                                                      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "#Model\n",
    "genre_embedding = GenreEncoder(num_genres, embedding_dim)\n",
    "model = FeatureEncoder(input_dim = X_train.shape[1], embedding_dim = embedding_dim, dropout_prob = dropout)\n",
    "#Loss\n",
    "criterion = nn.CosineSimilarity(dim=1)\n",
    "#Otimizer\n",
    "optimizer = optim.Adam( list(model.parameters()) + list(genre_embedding.parameters()), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    #ativa modo de treino nos dois modelos\n",
    "    model.train()\n",
    "    genre_embedding.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for x_feat, y_genre in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # z do género\n",
    "        z = genre_embedding ( y_genre )       # [batch, embedding_dim]\n",
    "        # ẑ das features\n",
    "        z_hat = model ( x_feat )  # [batch, embedding_dim]\n",
    "\n",
    "        # Loss = 1 - cos_sim\n",
    "        cos_sim = criterion( z_hat, z)\n",
    "        loss = 1 - cos_sim.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
