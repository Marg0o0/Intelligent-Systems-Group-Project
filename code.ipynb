{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "aec53e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import skfuzzy as fuzz\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d9e4643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 track_id                 artists  \\\n",
      "0  5SuOikwiRyPMVoIQDJUgSV             Gen Hoshino   \n",
      "1  4qPNDBW1i3p13qLCt0Ki3A            Ben Woodward   \n",
      "2  1iJBSr7s7jYXzM8EGcbK5b  Ingrid Michaelson;ZAYN   \n",
      "3  6lfxq3CG4xtTiEg7opyCyx            Kina Grannis   \n",
      "4  5vjLSffimiIP26QG5WcN2K        Chord Overstreet   \n",
      "\n",
      "                                          album_name  \\\n",
      "0                                             Comedy   \n",
      "1                                   Ghost (Acoustic)   \n",
      "2                                     To Begin Again   \n",
      "3  Crazy Rich Asians (Original Motion Picture Sou...   \n",
      "4                                            Hold On   \n",
      "\n",
      "                   track_name  popularity  duration_ms  explicit  \\\n",
      "0                      Comedy          73       230666     False   \n",
      "1            Ghost - Acoustic          55       149610     False   \n",
      "2              To Begin Again          57       210826     False   \n",
      "3  Can't Help Falling In Love          71       201933     False   \n",
      "4                     Hold On          82       198853     False   \n",
      "\n",
      "   danceability  energy  key  loudness  mode  speechiness  acousticness  \\\n",
      "0         0.676  0.4610    1    -6.746     0       0.1430        0.0322   \n",
      "1         0.420  0.1660    1   -17.235     1       0.0763        0.9240   \n",
      "2         0.438  0.3590    0    -9.734     1       0.0557        0.2100   \n",
      "3         0.266  0.0596    0   -18.515     1       0.0363        0.9050   \n",
      "4         0.618  0.4430    2    -9.681     1       0.0526        0.4690   \n",
      "\n",
      "   instrumentalness  liveness  valence    tempo  time_signature track_genre  \n",
      "0          0.000001    0.3580    0.715   87.917               4    acoustic  \n",
      "1          0.000006    0.1010    0.267   77.489               4    acoustic  \n",
      "2          0.000000    0.1170    0.120   76.332               4    acoustic  \n",
      "3          0.000071    0.1320    0.143  181.740               3    acoustic  \n",
      "4          0.000000    0.0829    0.167  119.949               4    acoustic  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(114000, 20)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Acesso ao dataset\n",
    "music = pd.read_csv('dataset.csv', index_col=0) #chama o ficheiro e remove coluna desnecess√°ria (index_col)\n",
    "\n",
    "#Garantir que o acesso foi bem sucedido (faz print das primeiras 5 linhas do dataset)\n",
    "print(music.head())\n",
    "music.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3a541e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antes da limpeza: 32656 m√∫sicas duplicadas\n",
      "(81344, 20)\n",
      "Depois de filtrar para incluir apenas as m√∫sicas: (80483, 20)\n",
      "track_id            0\n",
      "artists             1\n",
      "album_name          1\n",
      "track_name          1\n",
      "popularity          0\n",
      "duration_ms         0\n",
      "explicit            0\n",
      "danceability        0\n",
      "energy              0\n",
      "key                 0\n",
      "loudness            0\n",
      "mode                0\n",
      "speechiness         0\n",
      "acousticness        0\n",
      "instrumentalness    0\n",
      "liveness            0\n",
      "valence             0\n",
      "tempo               0\n",
      "time_signature      0\n",
      "track_genre         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##limpeza do dataset\n",
    "# Contar quantas linhas duplicadas existem (mesmo nome + mesmo artista)\n",
    "duplicadas_antes = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Antes da limpeza: {duplicadas_antes} m√∫sicas duplicadas\")\n",
    "\n",
    "# Remover duplicados (mesmo nome e mesmo artista)\n",
    "music = music.drop_duplicates(subset=[\"track_name\", \"artists\"], keep=\"first\")\n",
    "\n",
    "print(music.shape)\n",
    "\n",
    "#remover tracks que n√£o s√£o musica (podcasts, audio books)\n",
    "music = music[music['speechiness'] <= 0.66]\n",
    "\n",
    "print(f'Depois de filtrar para incluir apenas as m√∫sicas: {music.shape}')\n",
    "\n",
    "\n",
    "#valores em falta por coluna\n",
    "print(music.isnull().sum()) \n",
    "music = music.dropna()  #por serem poucos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1084acc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.715 0.267 0.12  ... 0.743 0.413 0.708]\n",
      "matriz x \n",
      " [[6.76000e-01 4.61000e-01 1.00000e+00 ... 1.01000e-06 8.79170e+01\n",
      "  4.00000e+00]\n",
      " [4.20000e-01 1.66000e-01 1.00000e+00 ... 5.56000e-06 7.74890e+01\n",
      "  4.00000e+00]\n",
      " [4.38000e-01 3.59000e-01 0.00000e+00 ... 0.00000e+00 7.63320e+01\n",
      "  4.00000e+00]\n",
      " ...\n",
      " [6.29000e-01 3.29000e-01 0.00000e+00 ... 0.00000e+00 1.32378e+02\n",
      "  4.00000e+00]\n",
      " [5.87000e-01 5.06000e-01 7.00000e+00 ... 0.00000e+00 1.35960e+02\n",
      "  4.00000e+00]\n",
      " [5.26000e-01 4.87000e-01 1.00000e+00 ... 0.00000e+00 7.91980e+01\n",
      "  4.00000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(80482, 20)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#defenir as features\n",
    "f0 = music['danceability'].values      #[0,1]\n",
    "f1 = music['energy'].values            #[0,1]\n",
    "f2 = music['key'].values               #[-1,11]\n",
    "f3 = music['loudness'].values          #dB\n",
    "f4 = music['mode'].values              #binary\n",
    "f5 = music['acousticness'].values      #[0,1]\n",
    "f6 = music['instrumentalness'].values  #[0,1]\n",
    "f7 = music['tempo'].values             #bpm\n",
    "f8 = music['time_signature'].values    #[0,5]\n",
    "\n",
    "f9 = music['track_genre'].values       #classes\n",
    "y = music['valence'].values         #valence-target\n",
    "\n",
    "'---------------------------------------'\n",
    "# matriz features\n",
    "#track genre n√£o incluida, s√≥ mais √° frente\n",
    "X = music[['danceability', 'energy', 'key', 'loudness', 'mode', 'acousticness', 'instrumentalness', 'tempo', 'time_signature']].values    \n",
    "\n",
    "#x = np.column_stack((f0, f1, f2, f3, f4, f5, f6, f7, f8))\n",
    "#x = np.c_[f0, f1, f2, f3, f4, f5, f6, f7, f8]\n",
    "\n",
    "'-------------------------------------melhor metodo?'\n",
    "\n",
    "print(y)\n",
    "print(f'matriz x \\n {X}')\n",
    "\n",
    "music.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1c21e9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Depois da limpeza: 0 m√∫sicas duplicadas\n"
     ]
    }
   ],
   "source": [
    "#verifica√ßa√µ da limpeza\n",
    "duplicadas_depois = music.duplicated(subset=[\"track_name\", \"artists\"]).sum()\n",
    "print(f\"Depois da limpeza: {duplicadas_depois} m√∫sicas duplicadas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e18f9b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test ser√° o conjunto nunca visto pelo algoritmo at√© ao fim'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train, validation e test split\n",
    "test_size = 0.3\n",
    "val_size = 0.5   #ser√° metade dos dados de teste\n",
    "#Seprarar o conjunto de treino (70%) e o tempor√°rio (30%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size = test_size, random_state = 42)\n",
    "#Separar o conjunto tempor√°rio em teste (15%) e valida√ß√£o (15%)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size = val_size, random_state = 42)\n",
    "\n",
    "'test ser√° o conjunto nunca visto pelo algoritmo at√© ao fim'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8f91ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4.97000000e-01 2.94000000e-01 4.54545455e-01 ... 0.00000000e+00\n",
      "  7.14926121e-01 4.00000000e+00]\n",
      " [5.86000000e-01 6.95000000e-01 9.09090909e-02 ... 0.00000000e+00\n",
      "  6.48883191e-01 4.00000000e+00]\n",
      " [6.67000000e-01 5.03000000e-01 0.00000000e+00 ... 0.00000000e+00\n",
      "  5.91748434e-01 4.00000000e+00]\n",
      " ...\n",
      " [7.88000000e-01 7.45000000e-01 1.81818182e-01 ... 8.50000000e-01\n",
      "  5.01294315e-01 4.00000000e+00]\n",
      " [5.64000000e-01 7.31000000e-01 8.18181818e-01 ... 3.48000000e-06\n",
      "  3.90496853e-01 4.00000000e+00]\n",
      " [7.71000000e-01 7.39000000e-01 8.18181818e-01 ... 0.00000000e+00\n",
      "  5.34260309e-01 4.00000000e+00]]\n",
      " matriz x-val: \n",
      " [[8.18000000e-01 7.09000000e-01 0.00000000e+00 ... 4.24000000e-02\n",
      "  5.25845208e-01 4.00000000e+00]\n",
      " [6.92000000e-01 8.57000000e-01 1.81818182e-01 ... 1.43000000e-01\n",
      "  4.49423105e-01 4.00000000e+00]\n",
      " [5.13000000e-01 9.16000000e-01 9.09090909e-02 ... 1.72000000e-05\n",
      "  5.01281988e-01 4.00000000e+00]\n",
      " ...\n",
      " [7.88000000e-01 7.45000000e-01 1.81818182e-01 ... 8.50000000e-01\n",
      "  5.01294315e-01 4.00000000e+00]\n",
      " [5.64000000e-01 7.31000000e-01 8.18181818e-01 ... 3.48000000e-06\n",
      "  3.90496853e-01 4.00000000e+00]\n",
      " [7.71000000e-01 7.39000000e-01 8.18181818e-01 ... 0.00000000e+00\n",
      "  5.34260309e-01 4.00000000e+00]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'----------PORQUE N√ÉO POSSO FAZER A NORMALIZA√á√ÉO ANTES DE SEPARAR ENTRE TESTE E TREINO\\nOU SEJA D√Å PARA O FAZER SEM LEVAR A OVER FITTING?\\n\\nSobre o coment√°rio (importante!)\\n\\n‚ÄúPor que n√£o posso fazer a normaliza√ß√£o antes de separar treino e teste?‚Äù\\n\\nPorque se fizeres isso, o MinMaxScaler vai usar informa√ß√£o do conjunto de valida√ß√£o/teste para definir os limites (m√≠nimo e m√°ximo).\\nIsso causa data leakage (vazamento de dados) e overfitting indireto, pois o modelo ‚Äúv√™‚Äù dados que n√£o deveria antes do treino.\\n\\nüëâ O correto √©:\\n\\nSeparar primeiro treino/valida√ß√£o/teste\\n\\nAjustar (fit) o scaler s√≥ no treino\\n\\nAplicar (transform) o mesmo scaler aos outros conjuntos.'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Normalizar varivaeis Key, loudness, tempo, time_signature\n",
    "scaler = MinMaxScaler()\n",
    "X_train[ :, [ 2, 3, 6, 7]] = scaler.fit_transform(X_train[ :, [ 2, 3, 6, 7]])\n",
    "X_val[ :, [ 2, 3, 6, 7]] = scaler.transform(X_val[ :, [2, 3, 6, 7]])\n",
    "\n",
    "print(X_train)\n",
    "print(f' matriz x-val: \\n {X_val}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f40501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreEncoder(nn.Module):\n",
    "    def __init__(self, num_genres, embedding_dim):\n",
    "        super(GenreEncoder, self).__init__()\n",
    "        self.genre_embedding = nn.Embedding(num_genres, embedding_dim)\n",
    "\n",
    "    def forward(self, genre_idx):\n",
    "        # genre_idx: [batch_size, 1] ou [batch_size]\n",
    "        z = self.embedding(genre_idx).squeeze(1)\n",
    "        return z  # z ‚àà R^(batch_size √ó embedding_dim)\n",
    "\n",
    "class FeatureEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, embedding_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_hat = self.model(x)\n",
    "        return z_hat  # zÃÇ ‚àà R^(batch_size √ó embedding_dim)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1253fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup\n",
    "embedding_dim = 30\n",
    "num_genres = 114\n",
    "input_dim = X_train.shape[1]\n",
    "num_epochs = 200\n",
    "learning_rate = 0.001\n",
    "dropout = 0.2     #nunca acima de 0.5\n",
    "batch_size = 64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7593c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model\n",
    "genre_embedding = GenreEncoder(num_genres, embedding_dim)\n",
    "model = FeatureEncoder(input_dim, embedding_dim)\n",
    "#Loss\n",
    "criterion = nn.CosineSimilarity(dim=1)\n",
    "#Otimizer\n",
    "optimizer = optim.Adam( list(model.parameters()) + list(genre_embedding.parameters()), lr=1e-3 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b265c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # z do g√©nero\n",
    "    z = genre_embedding ( x_genre )       # [batch, embedding_dim]\n",
    "    # zÃÇ das features\n",
    "    z_hat = model (x_feat)  # [batch, embedding_dim]\n",
    "\n",
    "    # Loss = 1 - cos_sim\n",
    "    cos_sim = criterion(z_hat, z)\n",
    "    loss = 1 - cos_sim.mean()\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368fa761",
   "metadata": {},
   "source": [
    "numero de layers ? como escolho\n",
    "posso usar o 64 tambem ou √© melhor optar por algo mais elabora devido a um data set maior\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0a515ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convers√£o dos dados para tensores pythorch\n",
    "X_train = torch.tensor (X_train, dtype = torch.float32)\n",
    "y_train = torch.tensor (y_train, dtype = torch.float32)\n",
    "X_val = torch.tensor (X_val, dtype = torch.float32)\n",
    "y_val = torch.tensor (y_val, dtype = torch.float32)\n",
    "\n",
    "#unir tensores X_train e y_train num dataset\n",
    "train_dataset = TensorDataset (X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
